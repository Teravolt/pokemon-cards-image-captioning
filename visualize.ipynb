{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize representation space of ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "CONFIG = Namespace(\n",
    "    run_name='CONTRAST-EXP-contrast-loss-full-final-layers-visualize',\n",
    "    seed=SEED,\n",
    "    generation_max_length=256,\n",
    "    generation_num_beams=1,\n",
    "    min_num_clusters=3,\n",
    "    max_num_clusters=50)\n",
    "\n",
    "run = wandb.init(project='pokemon-cards',\n",
    "                 entity=None,\n",
    "                 job_type='visualize',\n",
    "                 name=CONFIG.run_name)\n",
    "\n",
    "MODEL_ARTIFACT = './artifacts/pokemon-image-captioning-model:v14'\n",
    "if not os.path.exists(MODEL_ARTIFACT):\n",
    "    artifact = run.use_artifact('pkthunder/pokemon-cards/pokemon-image-captioning-model:v14', type='model')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "PRETRAINED_MODEL = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "PRETRAINED_MODEL.to(DEVICE)\n",
    "\n",
    "FINE_TUNED_MODEL = VisionEncoderDecoderModel.from_pretrained(MODEL_ARTIFACT)\n",
    "FINE_TUNED_MODEL.to(DEVICE)\n",
    "\n",
    "# Define image feature extractor and tokenizer\n",
    "# NOTE: these are not trained, so we can get them directly from HuggingFace\n",
    "FEATURE_EXTRACTOR = AutoFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(run):\n",
    "    \"\"\"\n",
    "    Download data from wandb\n",
    "    \"\"\"\n",
    "    \n",
    "    split_data_loc = run.use_artifact('pokemon_cards_split_full:v0')\n",
    "    table = split_data_loc.get(f\"pokemon_table_full_data_split_seed_{SEED}\")\n",
    "    return table\n",
    "\n",
    "def get_df(table, is_test=False):\n",
    "    \"\"\"\n",
    "    Get dataframe from wandb table\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = pd.DataFrame(data=table.data, columns=table.columns)\n",
    "\n",
    "    if is_test:\n",
    "        test_df = dataframe[dataframe.split == 'test']\n",
    "        return test_df\n",
    "\n",
    "    train_val_df = dataframe[dataframe.split != 'test']\n",
    "    return train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch], dim=0),\n",
    "        'labels': torch.stack([x['labels'] for x in batch], dim=0)\n",
    "    }\n",
    "\n",
    "class PokemonCardsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images:list, captions: list, config) -> None:\n",
    "\n",
    "        self.images = []\n",
    "        for image in images:\n",
    "            image_ = image.image\n",
    "            if image_.mode != \"RGB\":\n",
    "                image_ = image_.convert(mode=\"RGB\")\n",
    "            self.images.append(image_)\n",
    "\n",
    "        self.captions = captions\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        caption = self.captions[index]\n",
    "\n",
    "        pixel_values = FEATURE_EXTRACTOR(images=image, return_tensors=\"pt\").pixel_values[0]\n",
    "        tokenized_caption = TOKENIZER.encode(\n",
    "            caption, return_tensors='pt', padding='max_length',\n",
    "            truncation='longest_first',\n",
    "            max_length=self.config.generation_max_length)[0]\n",
    "\n",
    "        output = {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': tokenized_caption\n",
    "            }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed validation data\n",
    "wandb_table = download_data(run)\n",
    "train_val_df = get_df(wandb_table)\n",
    "val_df = train_val_df[train_val_df.split == 'valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass images through ViT and get contextualized embeddings from final layer\n",
    "# Take average of embeddings and visualize them.\n",
    "\n",
    "def get_embeddings(model, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get embeddings from final layer of ViT\n",
    "    \"\"\"\n",
    "\n",
    "    start_text = \"Pokemon Card of type\"\n",
    "    end_text = \"with the title\"\n",
    "\n",
    "    # start_text = 'of rarity'\n",
    "    # end_text = 'from the set'\n",
    "\n",
    "    embeddings = []\n",
    "    inst_labels = []\n",
    "    set_names = dataframe.set_name.values\n",
    "    card_names = dataframe.name.values\n",
    "\n",
    "    dataset = PokemonCardsDataset(\n",
    "        dataframe.image.values,\n",
    "        dataframe.caption.values,\n",
    "        CONFIG)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dataloader = DataLoader(dataset)\n",
    "        for i, inst in enumerate(dataloader):\n",
    "            enc_output = model.encoder(pixel_values=inst['pixel_values'].to(DEVICE))\n",
    "            # _embedding = enc_output.pooler_output\n",
    "\n",
    "            caption = dataframe.caption.values[i]\n",
    "            start_pos = caption.find(start_text)+len(start_text)\n",
    "            end_pos = caption.find(end_text, start_pos)\n",
    "            label = caption[start_pos:end_pos].strip()\n",
    "\n",
    "            # print(f\"Parsed label: {label}\")\n",
    "            # if 'evolved from' in label:\n",
    "            #     label = label[0:label.find('evolved from')]\n",
    "            #     label = label.strip()\n",
    "            # if ' ' in label:\n",
    "            #     label = label.split(' ')\n",
    "            #     label = label[1] if len(label[0]) == 1 else label[0]\n",
    "            # print(f\"Final label: {label}\")\n",
    "\n",
    "            inst_labels.append(label)\n",
    "\n",
    "            _embedding = enc_output.last_hidden_state.cpu()\n",
    "            _embedding = _embedding.mean(1)\n",
    "            # _embedding = _embedding.squeeze(0)\n",
    "            # inst_labels += [dataframe.set_name.values[i]]*_embedding.shape[0]\n",
    "            embeddings.append(_embedding)\n",
    "\n",
    "        embeddings = torch.concat(embeddings, axis=0)\n",
    "\n",
    "    return embeddings, inst_labels, set_names, card_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pretrained_embeddings, inst_labels, set_names, card_names = get_embeddings(PRETRAINED_MODEL, val_df)\n",
    "finetuned_embeddings, _, _, _ = get_embeddings(FINE_TUNED_MODEL, val_df)\n",
    "\n",
    "# L2 Norm the embeddings\n",
    "pretrained_embeddings = normalize(pretrained_embeddings)\n",
    "finetuned_embeddings = normalize(finetuned_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster embeddings via k-medoids \n",
    "\n",
    "Compute best number of cluster via silhouette score + elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "def run_kmedoids(embeddings, num_clusters=8):\n",
    "    \"\"\"\n",
    "    Train KMedoids\n",
    "    \"\"\"\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=num_clusters, metric='cosine')\n",
    "    kmedoids.fit(embeddings)\n",
    "    labels = kmedoids.predict(embeddings)\n",
    "\n",
    "    return kmedoids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "pt_silhouette_scores = []\n",
    "ft_silhouette_scores = []\n",
    "\n",
    "CONFIG.max_num_clusters = 50\n",
    "for n_clusters in range(CONFIG.min_num_clusters, CONFIG.max_num_clusters+1):\n",
    "\n",
    "    _, labels = run_kmedoids(pretrained_embeddings, num_clusters=n_clusters)\n",
    "    pt_avg_score = silhouette_score(pretrained_embeddings, labels, metric='cosine', random_state=SEED)\n",
    "    pt_silhouette_scores.append((n_clusters, pt_avg_score))\n",
    "\n",
    "    _, labels = run_kmedoids(finetuned_embeddings, num_clusters=n_clusters)\n",
    "    ft_avg_score = silhouette_score(finetuned_embeddings, labels, metric='cosine', random_state=SEED)\n",
    "    ft_silhouette_scores.append((n_clusters, ft_avg_score))\n",
    "\n",
    "print(ft_silhouette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2.5)\n",
    "plt.set_cmap('tab20')\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_df = pd.DataFrame(columns=['num_clusters', 'silhouette_score'])\n",
    "silhouette_df.num_clusters = list(range(CONFIG.min_num_clusters, CONFIG.max_num_clusters+1))\n",
    "silhouette_df.silhouette_score = [x[1] for x in ft_silhouette_scores]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(silhouette_df.num_clusters,\n",
    "        silhouette_df.silhouette_score)\n",
    "\n",
    "ax.set_xlabel(\"Number of Clusters\")\n",
    "ax.set_ylabel(\"Silhouette Score\")\n",
    "ax.set_ylim(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "pokemon_type_score = silhouette_score(finetuned_embeddings, inst_labels, metric='cosine', random_state=SEED)\n",
    "pokemon_type_score_by_sample = silhouette_samples(finetuned_embeddings, inst_labels, metric='cosine')\n",
    "\n",
    "print(f\"Silhouette score for Pokemon type clustering: {pokemon_type_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings from fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids_obj, labels = run_kmedoids(finetuned_embeddings, num_clusters=15)\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     finetuned_embeddings,\n",
    "#     columns=[f\"dim_{i}\" for i in range(finetuned_embeddings.shape[1])])\n",
    "# df['cluster'] = labels\n",
    "\n",
    "# run.log({'finetuned-embeddings': wandb.Table(dataframe=df)})\n",
    "\n",
    "color_map = plt.get_cmap('tab20')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_embedding = pca.fit_transform(finetuned_embeddings)\n",
    "df = pd.DataFrame(pca_embedding, columns=['pca1', 'pca2'])\n",
    "df['cluster'] = labels\n",
    "df['inst_label'] = inst_labels\n",
    "df['set_name'] = set_names\n",
    "df['name'] = card_names\n",
    "df['inst_label_silhouette'] = pokemon_type_score_by_sample\n",
    "\n",
    "plot_labels = inst_labels\n",
    "\n",
    "num_labels = len(set(plot_labels))\n",
    "for i, label in enumerate(set(plot_labels)):\n",
    "    label_df = df[df.inst_label == label]\n",
    "    ax.scatter(label_df.pca1, label_df.pca2, label=str(label), c=color_map(i))\n",
    "\n",
    "ax.set_xlabel(\"PCA Dimension 1\")\n",
    "ax.set_ylabel(\"PCA Dimension 2\")\n",
    "# ax.set_zlabel(\"PCA Dimension 3\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(inst_labels):\n",
    "    num_samples = df[df.inst_label == label].inst_label_silhouette.shape[0]\n",
    "    pokemon_type_score_ = df[df.inst_label == label].inst_label_silhouette.mean()\n",
    "    print(label, num_samples, pokemon_type_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print k-medoids centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medoid_indices = kmedoids_obj.medoid_indices_\n",
    "\n",
    "for i, index in enumerate(medoid_indices):\n",
    "    image = val_df.image.values[index].image\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(mode=\"RGB\")\n",
    "\n",
    "    image.save(f'centroid-{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valley of Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, labels = run_kmedoids(pretrained_embeddings, num_clusters=pt_num_clusters[0])\n",
    "\n",
    "# # df = pd.DataFrame(\n",
    "# #     pretrained_embeddings,\n",
    "# #     columns=[f\"dim_{i}\" for i in range(pretrained_embeddings.shape[1])])\n",
    "# # df['cluster'] = labels\n",
    "\n",
    "# # run.log({'pretrained-embeddings': wandb.Table(dataframe=df)})\n",
    "\n",
    "# fig = plt.figure(figsize=(30, 10))\n",
    "# ax = fig.add_subplot(111)\n",
    "# # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_embedding = pca.fit_transform(pretrained_embeddings)\n",
    "# df = pd.DataFrame(pca_embedding, columns=['pca1', 'pca2'])\n",
    "# df['cluster'] = labels\n",
    "# df['inst_labels'] = inst_labels\n",
    "\n",
    "# num_labels = len(set(labels))\n",
    "\n",
    "# for label in set(labels):\n",
    "#     label_df = df[df.cluster == label]\n",
    "#     ax.scatter(label_df.pca1, label_df.pca2, label=str(label))\n",
    "\n",
    "# ax.set_xlabel(\"PCA Dimension 1\")\n",
    "# ax.set_ylabel(\"PCA Dimension 2\")\n",
    "# # ax.set_zlabel(\"PCA Dimension 3\")\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "# plt.show()\n",
    "# # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as T\n",
    "\n",
    "# to_pil_img = T.ToPILImage()\n",
    "# to_tensor = T.ToTensor()\n",
    "\n",
    "# dataset = PokemonCardsDataset(\n",
    "#     val_df.image.values,\n",
    "#     val_df.caption.values,\n",
    "#     CONFIG)\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     dataloader = DataLoader(dataset)\n",
    "#     for i, inst in enumerate(dataloader):\n",
    "#         patches = to_tensor(dataset.images[i]).unfold(1, 128, 128).unfold(2, 128, 128)\n",
    "#         patches = patches.reshape(patches.shape[0], patches.shape[1]*patches.shape[2], patches.shape[3], patches.shape[4])\n",
    "#         patches = patches.transpose(0, 1)\n",
    "#         # unfold = torch.nn.Unfold(kernel_size=(16, 16), stride=16)\n",
    "#         # print(unfold(inst['pixel_values']).shape)\n",
    "#         # img = to_pil_img(inst['pixel_values'].squeeze(0))\n",
    "#         # print(PRETRAINED_MODEL.encoder.embeddings.patch_embeddings(inst['pixel_values']).shape)\n",
    "#         # patch = PRETRAINED_MODEL.encoder.embeddings.patch_embeddings.projection(inst['pixel_values'])\n",
    "#         for j in range(patches.shape[0]):\n",
    "#             patch = to_pil_img(patches[j])\n",
    "#             patch.save(f'patch-{j}-for-img-{i}.png')\n",
    "#         raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokemon-cards-image-captioning-IB8-aKYx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
