{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize latent space of Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "CONFIG = Namespace(\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    report_to='wandb',\n",
    "    run_name='fine_tuning_eval',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    output_dir='eval-output/',\n",
    "    optim='adamw_torch',\n",
    "    generation_max_length=256,\n",
    "    generation_num_beams=1,\n",
    "    log_preds=False,\n",
    "    val_limit=0,\n",
    "    min_num_clusters=3,\n",
    "    max_num_clusters=15\n",
    "    )\n",
    "\n",
    "run = wandb.init(project='pokemon-cards',\n",
    "                 entity=None,\n",
    "                 job_type='visualize',\n",
    "                 name='visualize_embedings')\n",
    "\n",
    "MODEL_ARTIFACT = './artifacts/pokemon-image-captioning-model:v10'\n",
    "\n",
    "if not os.path.exists(MODEL_ARTIFACT):\n",
    "    artifact = run.use_artifact('pkthunder/model-registry/Pokemon Card Image Captioner Full Dataset Model:v0', type='model')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PRETRAINED_MODEL = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "PRETRAINED_MODEL.to(DEVICE)\n",
    "\n",
    "FINE_TUNED_MODEL = VisionEncoderDecoderModel.from_pretrained(MODEL_ARTIFACT)\n",
    "FINE_TUNED_MODEL.to(DEVICE)\n",
    "\n",
    "# Define image feature extractor and tokenizer\n",
    "# NOTE: these are not trained, so we can get them directly from HuggingFace\n",
    "FEATURE_EXTRACTOR = AutoFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(run):\n",
    "    \"\"\"\n",
    "    Download data from wandb\n",
    "    \"\"\"\n",
    "    \n",
    "    split_data_loc = run.use_artifact('pokemon_cards_split_full:v0')\n",
    "    table = split_data_loc.get(f\"pokemon_table_full_data_split_seed_{SEED}\")\n",
    "    return table\n",
    "\n",
    "def get_df(table, is_test=False):\n",
    "    \"\"\"\n",
    "    Get dataframe from wandb table\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = pd.DataFrame(data=table.data, columns=table.columns)\n",
    "\n",
    "    if is_test:\n",
    "        test_df = dataframe[dataframe.split == 'test']\n",
    "        return test_df\n",
    "\n",
    "    train_val_df = dataframe[dataframe.split != 'test']\n",
    "    return train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch], dim=0),\n",
    "        'labels': torch.stack([x['labels'] for x in batch], dim=0)\n",
    "    }\n",
    "\n",
    "class PokemonCardsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images:list, captions: list, config) -> None:\n",
    "\n",
    "        self.images = []\n",
    "        for image in images:\n",
    "            image_ = image.image\n",
    "            if image_.mode != \"RGB\":\n",
    "                image_ = image_.convert(mode=\"RGB\")\n",
    "            self.images.append(image_)\n",
    "\n",
    "        self.captions = captions\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        caption = self.captions[index]\n",
    "\n",
    "        pixel_values = FEATURE_EXTRACTOR(images=image, return_tensors=\"pt\").pixel_values[0]\n",
    "        tokenized_caption = TOKENIZER.encode(\n",
    "            caption, return_tensors='pt', padding='max_length',\n",
    "            truncation='longest_first',\n",
    "            max_length=self.config.generation_max_length)[0]\n",
    "\n",
    "        output = {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': tokenized_caption\n",
    "            }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use validation dataset\n",
    "\n",
    "wandb_table = download_data(run)\n",
    "train_val_df = get_df(wandb_table)\n",
    "val_df = train_val_df[train_val_df.split == 'valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass images through ViT and get contextualized embeddings from final layer\n",
    "# Take average of embeddings and visualize them.\n",
    "\n",
    "def get_embeddings(model, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get embeddings from final layer of ViT\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    inst_labels = []\n",
    "\n",
    "    dataset = PokemonCardsDataset(\n",
    "        dataframe.image.values,\n",
    "        dataframe.caption.values,\n",
    "        CONFIG)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dataloader = DataLoader(dataset)\n",
    "        for i, inst in enumerate(dataloader):\n",
    "            enc_output = model.encoder(pixel_values=inst['pixel_values'])\n",
    "            # _embedding = enc_output.pooler_output\n",
    "            inst_labels.append(dataframe.set_name.values[i])\n",
    "            _embedding = enc_output.last_hidden_state\n",
    "            _embedding = _embedding.mean(1)\n",
    "            # _embedding = _embedding.squeeze(0)\n",
    "            # inst_labels += [dataframe.set_name.values[i]]*_embedding.shape[0]\n",
    "            embeddings.append(_embedding)\n",
    "\n",
    "            # print(enc_output.last_hidden_state.numpy().shape)\n",
    "        embeddings = torch.concat(embeddings, axis=0)\n",
    "    return embeddings, inst_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pretrained_embeddings, inst_labels = get_embeddings(PRETRAINED_MODEL, val_df)\n",
    "finetuned_embeddings, _ = get_embeddings(FINE_TUNED_MODEL, val_df)\n",
    "\n",
    "# L2 Norm the embeddings\n",
    "pretrained_embeddings = normalize(pretrained_embeddings)\n",
    "finetuned_embeddings = normalize(finetuned_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal number of clusters using silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "def run_kmedoids(embeddings, num_clusters=8):\n",
    "    \"\"\"\n",
    "    Train KMedoids\n",
    "    \"\"\"\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=num_clusters, metric='cosine')\n",
    "    kmedoids.fit(embeddings)\n",
    "    labels = kmedoids.predict(embeddings)\n",
    "\n",
    "    return kmedoids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "pt_silhouette_scores = []\n",
    "ft_silhouette_scores = []\n",
    "\n",
    "for n_clusters in range(CONFIG.min_num_clusters, CONFIG.max_num_clusters+1):\n",
    "\n",
    "    _, labels = run_kmedoids(pretrained_embeddings, num_clusters=n_clusters)\n",
    "    pt_avg_score = silhouette_score(pretrained_embeddings, labels, metric='cosine', random_state=SEED)\n",
    "    pt_silhouette_scores.append((n_clusters, pt_avg_score))\n",
    "\n",
    "    _, labels = run_kmedoids(finetuned_embeddings, num_clusters=n_clusters)\n",
    "    ft_avg_score = silhouette_score(finetuned_embeddings, labels, metric='cosine', random_state=SEED)\n",
    "    ft_silhouette_scores.append((n_clusters, ft_avg_score))\n",
    "\n",
    "pt_silhouette_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "ft_silhouette_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(pt_silhouette_scores)\n",
    "print(ft_silhouette_scores)\n",
    "\n",
    "pt_num_clusters = pt_silhouette_scores[0]\n",
    "ft_num_clusters = ft_silhouette_scores[0]\n",
    "\n",
    "print(f\"Pretrained num clusters: {pt_num_clusters}\")\n",
    "print(f\"Finetuned num clusters: {ft_num_clusters}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Embeddings from Pretrained and Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "_, labels = run_kmedoids(pretrained_embeddings, num_clusters=pt_num_clusters[0])\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_embedding = pca.fit_transform(pretrained_embeddings)\n",
    "df = pd.DataFrame(pca_embedding, columns=['pca1', 'pca2'])\n",
    "df['cluster'] = labels\n",
    "df['inst_labels'] = inst_labels\n",
    "\n",
    "num_labels = len(set(labels))\n",
    "\n",
    "for label in set(labels):\n",
    "    label_df = df[df.cluster == label]\n",
    "    ax.scatter(label_df.pca1, label_df.pca2, label=str(label))\n",
    "\n",
    "ax.set_xlabel(\"PCA Dimension 1\")\n",
    "ax.set_ylabel(\"PCA Dimension 2\")\n",
    "# ax.set_zlabel(\"PCA Dimension 3\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels = run_kmedoids(finetuned_embeddings, num_clusters=ft_num_clusters[0])\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_embedding = pca.fit_transform(finetuned_embeddings)\n",
    "df = pd.DataFrame(pca_embedding, columns=['pca1', 'pca2'])\n",
    "df['cluster'] = labels\n",
    "df['inst_labels'] = inst_labels\n",
    "\n",
    "num_labels = len(set(labels))\n",
    "for label in set(labels):\n",
    "    label_df = df[df.cluster == label]\n",
    "    ax.scatter(label_df.pca1, label_df.pca2, label=str(label))\n",
    "\n",
    "ax.set_xlabel(\"PCA Dimension 1\")\n",
    "ax.set_ylabel(\"PCA Dimension 2\")\n",
    "# ax.set_zlabel(\"PCA Dimension 3\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as T\n",
    "\n",
    "# to_pil_img = T.ToPILImage()\n",
    "# to_tensor = T.ToTensor()\n",
    "\n",
    "# dataset = PokemonCardsDataset(\n",
    "#     val_df.image.values,\n",
    "#     val_df.caption.values,\n",
    "#     CONFIG)\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     dataloader = DataLoader(dataset)\n",
    "#     for i, inst in enumerate(dataloader):\n",
    "#         patches = to_tensor(dataset.images[i]).unfold(1, 128, 128).unfold(2, 128, 128)\n",
    "#         patches = patches.reshape(patches.shape[0], patches.shape[1]*patches.shape[2], patches.shape[3], patches.shape[4])\n",
    "#         patches = patches.transpose(0, 1)\n",
    "#         # unfold = torch.nn.Unfold(kernel_size=(16, 16), stride=16)\n",
    "#         # print(unfold(inst['pixel_values']).shape)\n",
    "#         # img = to_pil_img(inst['pixel_values'].squeeze(0))\n",
    "#         # print(PRETRAINED_MODEL.encoder.embeddings.patch_embeddings(inst['pixel_values']).shape)\n",
    "#         # patch = PRETRAINED_MODEL.encoder.embeddings.patch_embeddings.projection(inst['pixel_values'])\n",
    "#         for j in range(patches.shape[0]):\n",
    "#             patch = to_pil_img(patches[j])\n",
    "#             patch.save(f'patch-{j}-for-img-{i}.png')\n",
    "#         raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokemon-cards-image-captioning-IB8-aKYx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
