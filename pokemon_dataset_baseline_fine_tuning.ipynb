{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a fine-tuning baseline for the Pokemon Cards Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "MODEL = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "FEATURE_EXTRACTOR = AutoFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "#ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL.to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_BLEU_METRIC = evaluate.load('google_bleu')\n",
    "PERPLEXITY = evaluate.load('perplexity', module_type='metric')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset from Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(project='pokemon-cards', entity=None, job_type=\"training\", name='fine-tuning')\n",
    "\n",
    "split_data_loc = run.use_artifact('pokemon_cards_split:latest')\n",
    "processed_dataset_dir = Path(split_data_loc.download())\n",
    "\n",
    "table = split_data_loc.get(f\"pokemon_table_1k_data_split_seed_{SEED}\")\n",
    "\n",
    "dataframe = pd.DataFrame(data=table.data, columns=table.columns)\n",
    "\n",
    "train_df = dataframe[dataframe.split.str.fullmatch('train')]\n",
    "val_df = dataframe[dataframe.split.str.fullmatch('valid')]\n",
    "test_df = dataframe[dataframe.split.str.fullmatch('test')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pytorch Pokemon Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PokemonCardsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images:list, captions: list) -> None:\n",
    "\n",
    "        self.images = []\n",
    "        for image in images:\n",
    "            image_ = image.image\n",
    "            if image_.mode != \"RGB\":\n",
    "                image_ = image_.convert(mode=\"RGB\")\n",
    "            self.images.append(image_)\n",
    "\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        caption = self.captions[index]\n",
    "\n",
    "        pixel_values = FEATURE_EXTRACTOR(images=image, return_tensors=\"pt\").pixel_values[0]\n",
    "        tokenized_caption = TOKENIZER.encode(\n",
    "            caption, return_tensors='pt', padding='max_length',\n",
    "            truncation='longest_first', max_length=256)[0]\n",
    "\n",
    "        output = {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': tokenized_caption\n",
    "            }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PokemonCardsDataset(train_df.image.values[0:256], train_df.caption.values[0:256])\n",
    "val_dataset = PokemonCardsDataset(val_df.image.values, val_df.caption.values)\n",
    "test_dataset = PokemonCardsDataset(test_df.image.values, test_df.caption.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_table = wandb.Table(columns=['step', 'pred_text', 'gt_text', 'google_bleu'])\n",
    "VAL_ITER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_obj: EvalPrediction):\n",
    "    global VAL_ITER\n",
    "\n",
    "    pred_ids = eval_obj.predictions\n",
    "    gt_ids = eval_obj.label_ids\n",
    "\n",
    "    pred_texts = TOKENIZER.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_texts = [text.strip() for text in pred_texts]\n",
    "\n",
    "    gt_texts = TOKENIZER.batch_decode(gt_ids, skip_special_tokens=True)\n",
    "    gt_texts = [[text.strip()] for text in gt_texts]\n",
    "\n",
    "    avg_google_bleu = []\n",
    "    for pred_text, gt_text in zip(pred_texts, gt_texts):\n",
    "        google_bleu_metric = \\\n",
    "            GOOGLE_BLEU_METRIC.compute(predictions=[pred_text], references=[gt_text])\n",
    "        metrics_table.add_data(VAL_ITER, pred_text, gt_text, google_bleu_metric['google_bleu'])\n",
    "        avg_google_bleu.append(google_bleu_metric['google_bleu'])\n",
    "\n",
    "    avg_google_bleu = {'avg_google_bleu': sum(avg_google_bleu)/len(avg_google_bleu)}\n",
    "    VAL_ITER += 1\n",
    "\n",
    "    return avg_google_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    report_to='wandb',\n",
    "    run_name='fine-tuning',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    output_dir='baseline-ft-model-output/',\n",
    "    optim='adamw_torch',\n",
    "    generation_max_length=256,\n",
    "    generation_num_beams=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch], dim=0),\n",
    "        'labels': torch.stack([x['labels'] for x in batch], dim=0)\n",
    "    }\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=MODEL,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=FEATURE_EXTRACTOR,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log({\"fine_tuning\": metrics_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TOKENIZER.batch_decode(MODEL.generate(val_dataset[10]['pixel_values'].unsqueeze(0))))\n",
    "# print(val_dataset.captions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb-course-trrCvNNg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
